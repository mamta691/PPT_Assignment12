{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd25dce-9276-4a8a-a929-435e9b114c0f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Naive Approach:\n",
    "\n",
    "# 1. What is the Naive Approach in machine learning?\n",
    "Naive in machine learning refers to the assumption that the features that go into the model are independent of each other123. Naive Bayes is called naive because it assumes that each input variable is independent2. Naive Bayes is a set of supervised learning algorithms that apply the Bayes' theorem3. It is essentially a \"classifier\" that helps you classify things based on a series of independent \"naive\" assumptions3.\n",
    "# 2. Explain the assumptions of feature independence in the Naive Approach.\n",
    "There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable.\n",
    "# 3. How does the Naive Approach handle missing values in the data?\n",
    "The Naive Bayes classifier can handle missing data differently depending on whether they exist in training or testing/classification instances. When classifying instances, the attribute with the missing value is simply not included in the probability calculation1. In general, you have a choice when handling missing values when training a naive Bayes classifier. You can choose to either omit records with any missing values or omit only the missing attributes1\n",
    "# 4. What are the advantages and disadvantages of the Naive Approach?\n",
    "Advantages :- When the independent assumption holds then this classifier gives outstanding accuracy. Easy to implement as only the probability is to be calculated It works well with high dimensions such as text classification. Disadvantages :- If the independent assumption does not hold then performance is very low.\n",
    "# 5. Can the Naive Approach be used for regression problems? If yes, how?\n",
    "Yes, the Naive Bayes algorithm can be used for regression problems. The Naive Bayes algorithm is a probabilistic algorithm that can be used for classification and regression problems. In regression problems, the Naive Bayes algorithm is used to predict the value of a continuous variable based on the values of other variables. The Naive Bayes algorithm assumes that the variables are independent of each other and that the distribution of the variables is normal1.\n",
    "\n",
    "The Naive Bayes algorithm can be applied to numeric prediction tasks by modeling the probability distribution of the target value with kernel density estimators1.\n",
    "# 6. How do you handle categorical features in the Naive Approach?\n",
    "The way to deal with categorical data is to create each category as a feature and with boolean values. Not only this way removes the limitation of categories for some of the libraries (no applicable here since you have written your own function) but also it's fast.\n",
    "# 7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
    "Ans. Applying Laplace smoothing gives the classifier more options to classify the probabilities over more diverse events. Key Takeaways Laplace Smoothing is a technique that removes the problem of zero probability in the Naive Bayes Algorithm.\n",
    "# 8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
    "The optimal threshold is the point that maximizes the TPR and minimizes FPR. We can sort of eye-ball it on the blue curve above where the TPR is ~0.65. Rather than eye-balling, we can also pick the threshold that maximizes the geometric mean of the specificity and sensitivity.\n",
    "# 9. Give an example scenario where the Naive Approach can be applied.\n",
    "\n",
    "It is used for Credit Scoring. It is used in medical data classification. It can be used in real-time predictions because Naïve Bayes Classifier is an eager learner. It is used in Text classification such as Spam filtering and Sentiment analysis.\n",
    "\n",
    "# KNN:\n",
    "\n",
    "# 10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
    "he k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\n",
    "\n",
    "In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor.\n",
    "\n",
    "# 11. How does the KNN algorithm work?\n",
    "Supervised learning classifier\n",
    "KNN algorithm is a supervised learning classifier that uses proximity or distance to assign a class label to a new data point based on the labels of its nearest neighbors123. It is a non-parametric algorithm, meaning it does not make any assumptions on the data distribution3. It is also a lazy learning algorithm, meaning it defers the computation until classification4. It works on the principle of information gain, finding the most suitable neighbors to predict an unknown value5.\n",
    "# 12. How do you choose the value of K in KNN?\n",
    "In general notion, k is chosen to be sqrt(n),where n is the number of data-points,not the features. But the only way to validate your model is by the error on test data.\n",
    "\n",
    "What I generally do is, choose few random data-point from the dataset,and then find the k nearest neighbours for them.Count the number of neighbors with the different classes from the selected point.If the number of neighbors with different classes is too high,then change the value of k.\n",
    "# 13. What are the advantages and disadvantages of the KNN algorithm?\n",
    "Advantages of K-NN Algorithm It is simple to implement. No training is required before classification. Disadvantages of K-NN Algorithm Can be cost-intensive when working with a large data set. A lot of memory is required for processing large data sets. Choosing the right value of K can be tricky.\n",
    "# 14. How does the choice of distance metric affect the performance of KNN?\n",
    "An incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. This makes a monumental impact to the output of the algorithm. KNN is extremely resource intensive for large datasets, because ss the number of data points and features increase, the required calculations increases exponentially.\n",
    "# 15. Can KNN handle imbalanced datasets? If yes, how?\n",
    "The k-nearest neighbor algorithm is not influenced by the size of the class, so unbalanced classes are not a problem for the algorithm1. However, there are methods that can improve kNN on imbalanced datasets. One such method is the evolutionary optimized feature and class weights kNN (FCWkNN)2. Another method is the mixed weighted KNN algorithm, which assigns each sample of datasets an inverse proportion weight and combines it with the distance weight3.\n",
    "\n",
    "# 16. How do you handle categorical features in KNN?\n",
    "KNN doesn't handle categorical features well, which is a fundamental weakness of the algorithm1. KNN doesn't work well when features are on different scales, especially when one of the 'scales' is a category label1. Among the three classification methods, only Kernel Density Classification can handle categorical variables in theory, while kNN and SVM are unable to be applied directly since they are based on the Euclidean distances2. To handle categorical variables in KNN, create dummy variables out of a categorical variable and include them instead of the original categorical variable3.\n",
    "# 17. What are some techniques for improving the efficiency of KNN?\n",
    "To improve the efficiency of KNN model, you can123:\n",
    "Add a preprocessing stage to make the final algorithm run with more efficient data and then improve the effect of classification.\n",
    "Use the B-kNN algorithm, a variation of kNN equipped with a two-fold scheme for preprocessing the training dataset to reduce the size and improve the efficiency of kNN while making little sacrifice of accuracy compared to kNN.\n",
    "Reduce the dimensionality of the data by using feature selection, feature extraction, and feature engineering. Feature selection involves selecting the most relevant and informative features for the model. Feature extraction involves transforming the original features into a lower-dimensional space, such as PCA or LDA.\n",
    "# 18. Give an example scenario where KNN can be applied.\n",
    "Voting system\n",
    "The kNN algorithm can be considered a voting system, where the majority class label determines the class label of a new data point among its nearest ‘k’ (where k is an integer) neighbors in the feature space. Imagine a small village with a few hundred residents, and you must decide which political party you should vote for.\n",
    "\n",
    "# Clustering:\n",
    "\n",
    "# 19. What is clustering in machine learning?\n",
    "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups.\n",
    "It is basically a collection of objects on the basis of similarity and dissimilarity between them. \n",
    "# 20. Explain the difference between hierarchical clustering and k-means clustering.\n",
    "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of ‘K’. \n",
    "\n",
    "Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster. \n",
    "\n",
    "Main differences between K means and Hierarchical Clustering are: \n",
    "\n",
    "k-means Clustering v/s Hierarchical Clustering\n",
    "k-means, using a pre-specified  number of clusters, the method  assigns records to each cluster to  find the mutually exclusive cluster  of spherical shape based on distance.\tHierarchical methods can be either divisive or agglomerative.\n",
    "K Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.\t\n",
    "In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.\n",
    "One can use median or mean as a cluster centre to represent each cluster.\tAgglomerative methods  begin with ‘n’ clusters and sequentially combine similar clusters until only one cluster is obtained.\n",
    "Methods used are normally less computationally intensive and are suited with very large datasets.\tDivisive methods work in the opposite direction, beginning with one cluster that includes all the records and Hierarchical methods are especially useful when the target is to arrange the clusters into a natural hierarchy.\n",
    "In K Means clustering, since one start with random choice of clusters, the results produced by running the algorithm many times may differ.\tIn Hierarchical Clustering, results are reproducible in Hierarchical clustering\n",
    "K- means clustering a simply a division of the set of data objects into non-overlapping subsets (clusters) such that each  data object is in exactly one subset).\tA hierarchical clustering is a set of nested clusters that are arranged as a tree.\n",
    "K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D,  sphere in 3D).\tHierarchical clustering don’t work  as well as, k means when the  shape of the clusters is hyper  spherical.\n",
    "Advantages: 1. Convergence is guaranteed. 2. Specialized to clusters of different sizes and shapes.\tAdvantages:  1 .Ease of handling of any forms of similarity or distance. 2. Consequently, applicability to any attributes types.\n",
    "Disadvantages: 1. K-Value is difficult to predict 2. Didn’t work well with global cluster.\tDisadvantage: 1. Hierarchical clustering requires the computation and storage of an n×n  distance matrix. For very large datasets, this can be expensive and slow\n",
    " \n",
    "# 21. How do you determine the optimal number of clusters in k-means clustering?\n",
    "The optimal number of clusters can be defined as follow:\n",
    "\n",
    "Compute clustering algorithm (e.g., k-means clustering) for different values of k. \n",
    "For instance, by varying k from 1 to 10 clusters.\n",
    "For each k, calculate the total within-cluster sum of square (wss).\n",
    "\n",
    "Plot the curve of wss according to the number of clusters k.\n",
    "\n",
    "The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n",
    "# 22. What are some common distance metrics used in clustering?\n",
    "To determine the distance between two clusters in hierarchical clustering, you need to use a distance metric12. The most common distance metric used is the Euclidean distance, which is the square root of the sum of the square differences12. However, for gene expression, correlation distance is often used. The distance between two vectors is 0 when they are perfectly correlated1. The distance metric is used in combination with a linkage criterion, which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets2.\n",
    "\n",
    "# 23. How do you handle categorical features in clustering?\n",
    "The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimize the clustering cost function.\n",
    "\n",
    "# 24. What are the advantages and disadvantages of hierarchical clustering?\n",
    "Easy to understand and easy to do… There are four types of clustering algorithms in widespread use: hierarchical clustering, k-means cluster analysis, latent class analysis, and self-organizing maps. ...\n",
    "… But rarely provides the best solution ...\n",
    "Arbitrary decisions ...\n",
    "Missing data ...\n",
    "Data types ...\n",
    "\n",
    "# 25. Explain the concept of silhouette score and its interpretation in clustering.\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "# 26. Give an example scenario where clustering can be applied.\n",
    "\n",
    "Some common applications for clustering include the following: market segmentation social network analysis search result grouping medical imaging image segmentation anomaly detection\n",
    "\n",
    "# Anomaly Detection:\n",
    "\n",
    "# 27. What is anomaly detection in machine learning?\n",
    "Anomaly detection is a process in machine learning that identifies data points, events, and observations that deviate from a data set’s normal behavior. It is used to detect any outliers or data that largely varies in range from the normal operating conditions. Anomalies are unusual data points which are significantly different to the wider trends in the rest of the data set. They are unexpected deviations from the expected outcome. Anomaly detection will usually lead to an intervention12\n",
    "\n",
    "# 28. Explain the difference between supervised and unsupervised anomaly detection.\n",
    "approach involved\n",
    "Anomaly detection can be either supervised or unsupervised123. In supervised anomaly detection, you need labelled training data where for each row you know if it is an outlier/anomaly or not1. In unsupervised anomaly detection, you do not pass any labelled values3. The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching2.\n",
    "\n",
    "# 29. What are some common techniques used for anomaly detection?\n",
    "Some popular techniques for anomaly detection include12:\n",
    "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data\n",
    "Neural Networks-Based\n",
    "Bayesian Networks Based\n",
    "Support Vector Machines Based\n",
    "# 30. How does the One-Class SVM algorithm work for anomaly detection?\n",
    "One-Class Support Vector Machine (SVM) is an unsupervised model for anomaly or outlier detection1. It learns the boundary for the normal data points and identifies the data outside the border to be anomalies1. For anomaly detection, a semi-supervised variant of one-class SVM exists, where only normal data is required for training before anomalies can be detected2. In theory, the one-class SVM could also be used in an unsupervised anomaly detection setup, where no prior training is conducted2.\n",
    "\n",
    "# 31. How do you choose the appropriate threshold for anomaly detection?\n",
    "Thresholding anomaly detection is a process that calculates thresholds using the error, which is the difference between the real values and the predicted ones1. The upper threshold is equal to mean + (5 * standard deviation), and the lower threshold is equal to mean - (5 * standard deviation)1. If the error exceeds the thresholds, it is marked as anomalous. To tune the anomaly detection threshold, a train set is constructed using a large sample of observations without anomalies, and a smaller sample of observations containing anomalies is used to construct a validation and test set2.\n",
    "\n",
    "# 32. How do you handle imbalanced datasets in anomaly detection?\n",
    "Balanced vs Imbalanced Dataset :\n",
    "\n",
    "Balanced Dataset: In a Balanced dataset, there is approximately equal distribution of classes in the target column.\n",
    "Imbalanced Dataset: In an Imbalanced dataset, there is a highly unequal distribution of classes in the target column.\n",
    "Let’s understand this with the help of an example :\n",
    "Example : Suppose there is a Binary Classification problem with the following training data:\n",
    "\n",
    "Total Observations : 1000  \n",
    "Target variable class is either  ‘Yes’ or ‘No’.\n",
    "Case 1:\n",
    "If there are 900 ‘Yes’ and 100 ‘No’ then it represents an Imbalanced dataset as there is highly unequal distribution of the two classes. .\n",
    "\n",
    "Case 2:\n",
    "If there are 550 ‘Yes’ and 450 ‘No’ then it represents a Balanced dataset as there is approximately equal distribution of the two classes.\n",
    "\n",
    "Hence, there is a significant amount of difference between the sample sizes of the two classes in an Imbalanced Dataset.\n",
    "\n",
    "Problem with Imbalanced dataset:\n",
    "\n",
    "Algorithms may get biased towards the majority class and thus tend to predict output as the majority class.\n",
    "Minority class observations look like noise to the model and are ignored by the model.\n",
    "Imbalanced dataset gives misleading accuracy score.\n",
    "Techniques to deal with Imbalanced dataset :\n",
    "\n",
    "Under Sampling :\n",
    "In this technique, we reduce the sample size of Majority class and try to match it with the sample size of Minority Class.\n",
    "Example :\n",
    "Let’s take an imbalanced training dataset with 1000 records.\n",
    "\n",
    "Before Under Sampling :\n",
    "\n",
    "Target class ‘Yes’ = 900 records\n",
    "Target class ‘No’ = 100 records\n",
    "After Under Sampling :\n",
    "\n",
    "Target class ‘Yes’ = 100 records\n",
    "Target class ‘No’ = 100 records\n",
    "Now, both classes have the same sample size.\n",
    "\n",
    "Pros :\n",
    "\n",
    "Low computation power  needed.\n",
    "Cons :\n",
    "\n",
    "Some important patterns might get lost due to dropping of records.\n",
    "Only  beneficial for huge datasets with millions of records.\n",
    "Note : Under Sampling should only be done when we have huge number of records.\n",
    "\n",
    "Over Sampling :\n",
    "In this technique, we increase the sample size of Minority class by replication and try to match it with the sample size of Majority Class.\n",
    "Example :\n",
    "Let’s take the same imbalanced training dataset with 1000 records.\n",
    "\n",
    "Before Over Sampling :\n",
    "\n",
    "Target class ‘Yes’ = 900 records\n",
    "Target class ‘No’ = 100 records\n",
    "After Over Sampling :\n",
    "\n",
    "Target class ‘Yes’ = 900 records\n",
    "Target class ‘No’ = 900 records\n",
    "Pros :\n",
    "\n",
    "Patterns are not lost which enhances the model performance.\n",
    "Cons :\n",
    "\n",
    "Replication of the data can lead to overfitting.\n",
    "High computation power  needed.\n",
    "# 33. Give an example scenario where anomaly detection can be applied.\n",
    "For example, a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer. The system may also automatically block a suspicious charge.\n",
    "\n",
    "# Dimension Reduction:\n",
    "\n",
    "# 34. What is dimension reduction in machine learning?\n",
    "Dimension reduction in machine learning is the process of reducing the number of features in a dataset while retaining as much information as possible12. It can be used to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data132. Dimension reduction techniques include feature selection, linear algebra methods, projection methods, and autoencoders32.\n",
    "\n",
    "# 35. Explain the difference between feature selection and feature extraction.\n",
    "Feature selection and feature extraction are completely different from each other1. Feature selection is about selecting the subset of the original feature set, whereas feature extraction creates new features123. Feature selection ranks the existing attributes according to their predictive significance, while feature extraction transforms the attributes23. The transformed attributes, or features, are linear combinations of the original attributes2.\n",
    "\n",
    "# 36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets123. It works by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible1. PCA considers the variance of each attribute and reduces the dimensionality of the data by finding the lower-dimensional surface to project the high-dimensional data2.\n",
    "\n",
    "# 37. How do you choose the number of components in PCA?\n",
    "To choose the number of components in PCA, you can12:\n",
    "    \n",
    "Conduct a PCA and select the first two or three principal components for visualization.\n",
    "\n",
    "Choose a subset k of those principal components that can explain the most variance, typically at least 90% of the variance.\n",
    "\n",
    "Use the option that allows you to set the variance of the input that is supposed to be explained by the generated components.\n",
    "Typically, we want the explained variance to be between 95–99%3.\n",
    "You can plot the cumulative explained variance against the number of components to help you choose the number of components4.\n",
    "\n",
    "# 38. What are some other dimension reduction techniques besides PCA?\n",
    "\n",
    "Among machine learning algorithms, there are many basic linear dimension reduction methods including PCA, ICA, linear discriminant analysis (LDA), LFA, and LPP, but most of them are based on the projection of data, which makes data from the dimension reduction difficult to interpret.\n",
    "\n",
    "# 39. Give an example scenario where dimension reduction can be applied.\n",
    "E-mail classification problem\n",
    "An intuitive example of dimensionality reduction can be discussed through a simple e-mail classification problem, where we need to classify whether the e-mail is spam or not. This can involve a large number of features, such as whether or not the e-mail has a generic title, the content of the e-mail, whether the e-mail uses a template, etc.\n",
    "\n",
    "\n",
    "# Feature Selection:\n",
    "\n",
    "# 40. What is feature selection in machine learning?\n",
    "Feature Selection is the process of reducing the input variable to your model by using only relevant data and getting rid of noise in data1234. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve1. There are several methods for feature selection, including:\n",
    "Univariate Selection\n",
    "Recursive Feature Elimination\n",
    "Principal Component Analysis5.\n",
    "# 41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model.\n",
    "\n",
    "\n",
    "# 42. How does correlation-based feature selection work?\n",
    "Correlation-based feature selection (CFS) is a filter approach that evaluates feature subsets based on data intrinsic properties, specifically correlations1. It is an algorithm that couples an evaluation formula with an appropriate correlation measure and a heuristic search strategy2. CFS is used for CT images to determine the best feature subset and computes the correlation between two random variables3.\n",
    "\n",
    "# 43. How do you handle multicollinearity in feature selection?\n",
    "How to deal with Multicollinearity when selecting features?\n",
    "\n",
    "1. Understand the extent of multicollinearity Start by calculating correlation coefficients or variance inflation factors (VIF) to identify highly correlated features. ...\n",
    "2. Assess feature importance ...\n",
    "3. Remove redundant features ...\n",
    "4. Feature transformation ...\n",
    "5. Regularization techniques ...\n",
    "\n",
    "# 44. What are some common feature selection metrics?\n",
    "Common measures include the mutual information, the pointwise mutual information, Pearson product-moment correlation coefficient, Relief-based algorithms, and inter/intra class distance or the scores of significance tests for each class/feature combinations.\n",
    "\n",
    "# 45. Give an example scenario where feature selection can be applied.\n",
    "Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.\n",
    "\n",
    "# Data Drift Detection:\n",
    "\n",
    "# 46. What is data drift in machine learning?\n",
    "Data drift is a phenomenon in machine learning where the statistical properties of the data used to train a model change over time1. This can lead to a decrease in model accuracy over time21. Machine learning models are trained with historical data, but once they are used in the real world, they may become outdated and lose their accuracy over time due to data drift1. Monitoring data drift helps detect these model performance issues2.\n",
    "\n",
    "# 47. Why is data drift detection important?\n",
    "Drift detection is a critical component of resource lifecycle management in Infrastructure as Code (IaC) solutions1. Drift refers to cases where the infrastructure state changes and doesn’t match the one defined in the code1. Drift detection is important to ensure the quality of machine learning models in production2. The results of a drift detection tool should be reported in a developer-friendly manner to help developers quickly understand the issue and fix it in their deployed infrastructure3.\n",
    "\n",
    "# 48. Explain the difference between concept drift and feature drift.\n",
    "Label drift – a change in the probability of a label p (Y). Feature drift – a change in the probability of p (X), meaning there was a change in the distribution of the model’s input.\n",
    "\n",
    "\n",
    "# 49. What are some techniques used for detecting data drift?\n",
    "How can Drifts be Detected?\n",
    "\n",
    "Sequential analysis methods like DDM (drift detection method)/EDDM (early DDM) that depend on the error rate to spot drifts.\n",
    "\n",
    "Model-based methods that use custom models for drift detection.\n",
    "\n",
    "Time distribution-based methods that utilize statistical distance calculation methods which help identify drifts between probability distribution.\n",
    "\n",
    "# 50. How can you handle data drift in a machine learning model?\n",
    "Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift.\n",
    "\n",
    "# Data Leakage:\n",
    "\n",
    "# 51. What is data leakage in machine learning?\n",
    "Data leakage (also known as target leakage) in machine learning is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment1. Data leakage occurs when some information is fed to the model during the time of training which might not be available when the model is used to get predictions in real life2. This mostly occurs when a feature which directly or indirectly depends on the target variable is used to train the model2.\n",
    "\n",
    "# 52. Why is data leakage a concern?\n",
    "Data leakage is a serious concern for both individuals and businesses. \n",
    "It can lead to identity theft, fraud, financial loss for individuals and damage to the company’s reputation or giving competitors an advantage for businesses. It can also cause declining revenue, tarnished reputation, massive financial penalties, crippling lawsuits, legal action and litigation, operational downtime, lost revenue, brand value and reputation and financial, reputational and legal repercussions1.\n",
    "\n",
    "In machine learning, data leakage may cause overly optimistic or invalid predictive models2.\n",
    "# 53. Explain the difference between target leakage and train-test contamination.\n",
    "Target leakage\n",
    "\n",
    "A target leak occurs when your predictors include data that will not be available at the time you make the predictions. It’s important to think of the target leak in terms of the timing or chronological order of data availability, and not just whether a feature makes good predictions.\n",
    "\n",
    "Train-Test Contamination\n",
    "\n",
    "A different type of leak occurs when you are not careful to distinguish training data from validation data. Validation is meant to be a measure of how well the model performs on data it has not previously considered. You can subtly corrupt this process if the validation data affects preprocessing behaviour. This is referred to as train-test contamination.\n",
    "\n",
    "Data Leakage in Action\n",
    "Here you will learn one way to detect and remove target leaks. I will use credit card apps dataset and ignore the master data setup code. The result is that the information about each credit card application is stored in an X DataFrame. I will use it to predict which applications have been accepted in a y series. You can download the dataset from here:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'], false_values = ['no'])\n",
    "\n",
    "# Select target\n",
    "y = data.card\n",
    "\n",
    "# Select predictors\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "print(\"Number of rows in the dataset:\", X.shape[0])\n",
    "X.head()\n",
    "Code language: PHP (php)\n",
    "Number of rows in the dataset: 1319\n",
    "# 54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "\n",
    "Here are some best practices for avoiding data leakage in machine learning:\n",
    "\n",
    "Understand the problem domain: It is important to understand the problem domain and the potential sources of data leakage. \n",
    "\n",
    "For example, if you are working on a credit risk model, you should be aware of the regulations and policies related to credit decisions to ensure that your model is fair and unbiased.\n",
    "\n",
    "Carefully design your data pipeline: Carefully designing your data pipeline can help prevent data leakage. This includes selecting appropriate features, properly splitting your data, and ensuring that preprocessing steps are applied only to the training set.\n",
    "\n",
    "Use proper data splitting: Proper data splitting is essential for preventing data leakage. Use techniques such as stratified sampling to ensure that the distribution of the target variable is consistent across the training, validation, and test sets.\n",
    "\n",
    "Use cross-validation: Cross-validation is a powerful technique for detecting overfitting and data leakage. Use it to validate the performance of your model and detect any potential sources of data leakage.\n",
    "\n",
    "Regularize your model: Regularization can help prevent overfitting and reduce the model’s reliance on specific features or subsets of the data.\n",
    "\n",
    "Monitor your model’s performance: Continuously monitoring your model’s performance can help you detect any changes in its behavior and identify potential sources of data leakage.\n",
    "\n",
    "Validate your model in production: Validate your model in a production environment to ensure that it performs as expected and that there are no sources of data leakage.\n",
    "\n",
    "# 55. What are some common sources of data leakage?\n",
    "The most common causes of data leakage are123:\n",
    "    \n",
    "Human error, such as employees sending emails containing critical information to the wrong recipients, flaws in security policies, sensitive data left exposed due to unpatched vulnerabilities in the software, etc.2\n",
    "Misconfigurations, deliberate or accidental actions by insiders, and system errors3.\n",
    "Bad infrastructure, such as systems that are not configured properly or not secured properly1.\n",
    "Social engineering scams1.\n",
    "Poor password policies1.\n",
    "# 56. Givean example scenario where data leakage can occur.\n",
    "Common examples of these leaks are: Thefts of confidential documents. Pictures of sensitive info. Transferring data onto a USB drive. Sending files to a personal Dropbox account.\n",
    "\n",
    "# Cross Validation:\n",
    "\n",
    "# 57. What is cross-validation in machine learning?\n",
    "Cross validation in machine learning is a technique to test a model's performance or accuracy on unseen data123. It involves splitting the data into multiple portions and using some to train the model and others to evaluate it14. Cross validation helps to assess how well a model can generalize to new and unknown datasets, which is an essential goal in machine learning development235.\n",
    "\n",
    "# 58. Why is cross-validation important?\n",
    "Cross-validation is an invaluable tool for data scientists1. It is used to build more accurate machine learning models and evaluate how well they work on an independent test dataset1. When you use cross-validation in machine learning, you verify how accurate your model is on multiple and different subsets of data, ensuring that it generalizes well to the data that you collect in the future2. It improves the accuracy of the model2.\n",
    "\n",
    "# 59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "KFold is a cross-validator that divides the dataset into k folds. StratifiedKFold is a variation of KFold that ensures that each fold of dataset has the same proportion of observations with a given label1. Therefore, we should prefer StratifiedKFold over KFold when dealing with classifications tasks with imbalanced class distributions1. Stratified k-fold cross-validation is same as just k-fold cross-validation, but it does stratified sampling instead of random sampling2.\n",
    "\n",
    "# 60. How do you interpret the cross-validation results?\n",
    "Have a look at the distribution of the errors resulting from the CV - what does it look like? If it shows a standard deviation it could mean that your model is overfitting the training data. If not then your model has probably done a good job at generalizing. So now you have an idea of how well your model generalized to new, unseen data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b93b5-9b2a-46c9-992a-f88002a0e23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
